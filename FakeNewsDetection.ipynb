{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "zycKXzT3VWG4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU8Wn5y0Vbfj",
        "outputId": "64e77013-89e5-4e3b-d409-1948757766cd"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Tx_wxsTGWDkr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    tokens = word_tokenize(re.sub('[^a-zA-Z]', ' ', text.lower()))\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(lemmatized)"
      ],
      "metadata": {
        "id": "evtZejIgWFG7"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = pd.read_csv('/content/WELFake_Dataset.csv')"
      ],
      "metadata": {
        "id": "KJrOKGMdWHPU"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset['content'] = news_dataset['title'] + ' ' + news_dataset['text']"
      ],
      "metadata": {
        "id": "-ChBp3NAWLnO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwcthT8DW0FA",
        "outputId": "5aca7f31-b1c5-47a2-fcbf-a9c3e0c33a28"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying lemmatization. This might take some time...\")\n",
        "news_dataset['content'] = news_dataset['content'].apply(process_text)\n",
        "print(\"Lemmatization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ILu_UKWMB8",
        "outputId": "7836f8d9-0786-41cb-92e5-b7744f87df31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying lemmatization. This might take some time...\n",
            "Lemmatization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset.to_csv('/content/preprocessed_dataset.csv', index=False)\n",
        "print(\"Preprocessed data has been saved to 'preprocessed_news_dataset.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmyZomDRWN9X",
        "outputId": "9a6824fe-7d9d-4ba2-a143-d64072e5acd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data has been saved to 'preprocessed_news_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/preprocessed_dataset.csv')"
      ],
      "metadata": {
        "id": "PvENfAXRYj8T"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1mZQ-a-Z6p3",
        "outputId": "743f097d-9c5b-4da2-a874-8c8082e0d121"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              title  \\\n",
            "0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
            "1           1                                                NaN   \n",
            "2           2  UNBELIEVABLE! OBAMAâ€™S ATTORNEY GENERAL SAYS MO...   \n",
            "3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
            "4           4  SATAN 2: Russia unvelis an image of its terrif...   \n",
            "\n",
            "                                                text  label  \\\n",
            "0  No comment is expected from Barack Obama Membe...      1   \n",
            "1     Did they post their votes for Hillary already?      1   \n",
            "2   Now, most of the demonstrators gathered last ...      1   \n",
            "3  A dozen politically active pastors came here f...      0   \n",
            "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1   \n",
            "\n",
            "                                             content  \n",
            "0  law enforcement high alert following threat co...  \n",
            "1                                                NaN  \n",
            "2  unbelievable obama attorney general say charlo...  \n",
            "3  bobby jindal raised hindu us story christian c...  \n",
            "4  satan russia unvelis image terrifying new supe...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['content'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPJ-pn2laANk",
        "outputId": "f4412b5d-8708-443b-9441-ba496bde932a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['content'] = data['content'].fillna('No content available')"
      ],
      "metadata": {
        "id": "SoQ4WX__aNWi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['content'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D3wOouRa0Yj",
        "outputId": "c895a19c-4e68-4948-b73f-b1f4c93dadee"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "X = vectorizer.fit_transform(data['content']).toarray()\n",
        "\n",
        "print(X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl3g4aY7a2YC",
        "outputId": "2bb7bca4-f3bd-499d-b94a-e3946b753469"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72134, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['label'].values\n",
        "\n",
        "print(y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0Hbhw7a51-",
        "outputId": "297eb307-5c62-4d12-9f69-1943cd2858aa"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 0 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEzzg7uIbE-a",
        "outputId": "98c01881-5c58-4cbb-eea2-7384848902db"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (57707, 5000), Testing set shape: (14427, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "print(f\"Logistic Regression Accuracy: {log_reg_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkQxZRYubHp4",
        "outputId": "3b508529-9cb4-4410-e2da-fc2463fca62f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 94.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "naive_bayes = MultinomialNB()\n",
        "\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "y_pred_naive_bayes = naive_bayes.predict(X_test)\n",
        "\n",
        "naive_bayes_accuracy = accuracy_score(y_test, y_pred_naive_bayes)\n",
        "print(f\"Naive Bayes Accuracy: {naive_bayes_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4XwKF5PbLdu",
        "outputId": "d90b5a20-1a83-42c1-d2e4-34803f12f50b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 84.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if log_reg_accuracy > naive_bayes_accuracy:\n",
        "    print(\"Logistic Regression is the best model.\")\n",
        "    best_model = log_reg\n",
        "else:\n",
        "    print(\"Naive Bayes is the best model.\")\n",
        "    best_model = naive_bayes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCf7l-HXbZ_l",
        "outputId": "f2fad39e-f526-4eab-c931-dbc7194474f1"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression is the best model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "-05saIxebwnl"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/best_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_model, model_file)\n",
        "\n",
        "with open('/content/vectorizer.pkl', 'wb') as vectorizer_file:\n",
        "    pickle.dump(vectorizer, vectorizer_file)"
      ],
      "metadata": {
        "id": "BexO6Wh8cYNq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/best_model.pkl', 'rb') as model_file:\n",
        "    best_model = pickle.load(model_file)\n",
        "\n",
        "with open('/content/vectorizer.pkl', 'rb') as vectorizer_file:\n",
        "    vectorizer = pickle.load(vectorizer_file)"
      ],
      "metadata": {
        "id": "rrCwo-fHcfPS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_article = input(\"Enter the new article text to classify as Real or Fake: \")\n",
        "\n",
        "new_article_transformed = vectorizer.transform([new_article]).toarray()\n",
        "\n",
        "prediction = best_model.predict(new_article_transformed)\n",
        "\n",
        "print(f\"Prediction: {'Real' if prediction == 1 else 'Fake'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVTxDCYHciQW",
        "outputId": "8e76c2d0-b483-4f3e-a5a2-5de026dfa353"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the new article text to classify as Real or Fake: america give grand piano horse wednesday november lucas wilde america give grand piano horse america given grand piano horse expecting quality tune particularly looking forward beethoven ninth beamed horse supporter piano enthusiast jay cooper horse never given piano frankly establishment allow last change come america change better lot doubter doubter soon silenced graceful note chopin mozart maybe even little richard horse dobbin williams said really sure expected horse absolutely qualified play piano mean look hoof way general even sit chair properly earth anyone think good idea cooper grinned made piano great democrat elizabeth king said wanted get pianist low medium standard piano thumped anything exciting would perfectly reasonable background music people spoken people wanted horse god bless america get best newsthump story mailbox every friday free currently witterings add\n",
            "Prediction: Real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmIfPcaC48WC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}